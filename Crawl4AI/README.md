# ğŸ•·ï¸ Crawl4AI Tutorial

## ğŸ“Œ What is Crawl4AI?
**Crawl4AI** is an open-source web crawling framework designed specifically for extracting web content and formatting it for Large Language Models (LLMs). It converts messy HTML pages into structured, readable Markdown.

## ğŸš€ Why Crawl4AI?
- **ğŸ§  Built for LLMs**: Produces clean, RAG/fine-tune-ready Markdown.
- **âš¡ Lightning Fast**: Up to 6x faster performance.
- **ğŸŒ Flexible Browser Control**: Includes session management, proxies, and custom hooks.
- **ğŸ” Heuristic Intelligence**: Smart extraction with reduced model cost.
- **ğŸ“¦ Open Source**: Fully open-source and Docker/cloud ready.
- **ğŸŒ Thriving Community**: Actively maintained, trending on GitHub.

## ğŸ§ª Basic Crawl4AI Example - Single Page Crawl

```bash
pip install -U crawl4ai
crawl4ai-setup
```

```python
import asyncio
from crawl4ai import *

async def main():
    async with AsyncWebCrawler() as crawler:
        result = await crawler.arun(url="https://ai.pydantic.dev/")
        print(result.markdown)

if __name__ == "__main__":
    asyncio.run(main())
```

## ğŸ¤ Ethics of Web Scraping
Always respect `robots.txt` rules of a website (e.g., `youtube.com/robots.txt`). Scraping responsibly is essential!

## ğŸŒ Crawling Multiple Pages via Sitemap

```python
# Fetch sitemap, extract URLs and crawl each
# See full tutorial code for details
```

## âš¡ Fast Parallel Page Crawling

To maximize speed, Crawl4AI allows parallel crawling.

```bash
pip install psutil
```

```python
# Parallel crawling code that checks memory and reuses browser sessions
# See full tutorial code for details
```

## ğŸ“ Output
Crawled content is converted to Markdown, ideal for downstream tasks like summarization, training, or search.

---

**Made with â¤ï¸ using medrhouma**.
